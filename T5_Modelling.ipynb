{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This is a code by Sergi Abashidze, Paula GarcÃ­a, Reem Hageali and Sidhant Singhal._\n",
    "\n",
    "Inspired by: https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import necessary libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece==0.1.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572564431
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "import transformers\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration,Adafactor\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load training data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572564893
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Clean and treat training data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572565141
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# convert all text to lower case\n",
    "traindata.name = traindata.name.str.lower()\n",
    "traindata.description = traindata.description.str.lower()\n",
    "# removing both the leading and the trailing characters\n",
    "traindata.name = traindata.name.str.strip()\n",
    "traindata.description = traindata.description.str.strip()\n",
    "# remove \".\" from data\n",
    "traindata.name = traindata.name.str.replace(\".\", \"\")\n",
    "traindata.description = traindata.description.str.replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572565429
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "max_token_desc = max(traindata.description.apply(lambda x: len(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572565429
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print('The maximum number of tokens in the descriptions are', max_token_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Vocabulary from the tokens in training data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572567596
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the fullvocab object\n",
    "fullvocab = Counter()\n",
    "# update it by adding the product names\n",
    "for line in traindata.name:\n",
    "    fullvocab.update(line.split(\" \"))\n",
    "# update it by adding the product descriptions\n",
    "for line in traindata.description:\n",
    "    fullvocab.update(line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572570850
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fullvocab = Vocab(fullvocab, min_freq = 336)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the tokenizer object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572575632
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load pre-trained model tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "original_tokens = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572575632
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# add the tokens from training data\n",
    "words_to_add = list(fullvocab.stoi.keys())[2:]\n",
    "\n",
    "addwords = []\n",
    "for word in words_to_add:\n",
    "    if word not in tokenizer.get_vocab():\n",
    "        addwords.append(word)\n",
    "# if token not in tokenizer, we add it\n",
    "tokenizer.add_tokens(addwords)\n",
    "\n",
    "extended_tokens = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572571138
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f'There were {original_tokens} tokens originally. By adding the training data tokens, we now have {extended_tokens} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create our own custom dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572606239
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the object that holds the description tokens (input)\n",
    "encodedesc = tokenizer.batch_encode_plus(traindata.description, max_length= 512, padding='max_length',\\\n",
    "                                         return_tensors='pt', truncation = True)\n",
    "\n",
    "# extract the input ids\n",
    "input_ids = encodedesc[\"input_ids\"]\n",
    "\n",
    "# extract the input masks\n",
    "input_masks = encodedesc[\"attention_mask\"]\n",
    "\n",
    "# delete the object, as it is no longer needed\n",
    "del encodedesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618555644808
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print('The shape of the input ids is',input_ids.shape)\n",
    "print('The shape of the input masks is',input_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572625007
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the object that holds the names tokens (target)\n",
    "encodednames = tokenizer.batch_encode_plus(traindata.name, max_length= 45, padding='max_length',\\\n",
    "                                           return_tensors='pt', truncation = True)\n",
    "\n",
    "# extract the target ids\n",
    "targ_ids = encodednames[\"input_ids\"]\n",
    "\n",
    "# extract the target masks\n",
    "targ_masks = encodednames[\"attention_mask\"]\n",
    "\n",
    "# delete the object, as it is no longer needed\n",
    "del encodednames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the target ids is',targ_ids.shape)\n",
    "print('The shape of the target masks is',targ_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572625175
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, descid, descmask, targid, targmask):\n",
    "        self.descid = descid\n",
    "        self.descmask = descmask\n",
    "        self.targid = targid\n",
    "        self.targmask = targmask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descid)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'source_ids': self.descid[index].to(dtype=torch.long), \n",
    "            'source_mask': self.descmask[index].to(dtype=torch.long), \n",
    "            'target_ids': self.targid[index].to(dtype=torch.long),\n",
    "            'target_mask': self.targmask[index].to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572635573
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load pre-trained model: \"t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "# set the model in training mode\n",
    "model.train()\n",
    "# resize the token embeddings to the updated_tokenizer we just created\n",
    "model.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train-Test Split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572636165
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# define the split: 80-20\n",
    "seperator = int(len(traindata)*0.8)\n",
    "allindexes = np.random.permutation([i for i in range(len(traindata))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572636165
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# define training set\n",
    "traindexes = allindexes[:seperator]\n",
    "training_set = CustomDataset(input_ids[traindexes, :], input_masks[traindexes, :],\\\n",
    "                             targ_ids[traindexes, :], targ_masks[traindexes,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572636464
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# define validation set\n",
    "valdexes = allindexes[seperator:]\n",
    "val_set = CustomDataset(input_ids[valdexes, :], input_masks[valdexes, :],\\\n",
    "                        targ_ids[valdexes, :], targ_masks[valdexes,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Loading__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_params = {'batch_size': 4,'shuffle': True,'num_workers': 0}\n",
    "training_loader = torch.utils.data.DataLoader(training_set, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "val_params = {'batch_size': 4,'shuffle': False,'num_workers': 0}\n",
    "val_loader = torch.utils.data.DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model on our data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572637045
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids']\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids']\n",
    "        mask = data['source_mask']\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if _%100 == 0 and _>0:\n",
    "            print(time.time())\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572637606
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params =  model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618569692945
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(2)):\n",
    "    train(epoch, tokenizer, model, training_loader, optimizer)\n",
    "    torch.save(model.state_dict(), \"T5_Epochs\"+ str(epoch) +\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572663043
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load the state dictionary we just created\n",
    "model.load_state_dict(torch.load(\"T5_Epochs0.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validate the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618573274506
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, loader):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids']\n",
    "            ids = data['source_ids']\n",
    "            mask = data['source_mask']\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids, attention_mask = mask, max_length=50, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, length_penalty=3, early_stopping=False)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
    "            \n",
    "            if _%100==0 and _>10:\n",
    "                print(f'Completed {_}')\n",
    "                \n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618572663437
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618575946131
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "predictions, actuals = validate(1, tokenizer, model, val_loader)\n",
    "final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the Rouge score to our model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618577637738
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618577812400
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# identify non-problematic records\n",
    "nonproblems = []\n",
    "for i in range(len(predictions)):\n",
    "    try:\n",
    "        rouge.get_scores(predictions[i], actuals[i], avg = True, ignore_empty = True)\n",
    "        nonproblems.append(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618577987430
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# select non-problematic records\n",
    "fullpreds = []\n",
    "fullactuals = []\n",
    "for i in nonproblems:\n",
    "    fullpreds.append(predictions[i])\n",
    "    fullactuals.append(actuals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618578003289
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get the rouge score for those non-problematic records\n",
    "rouge.get_scores(fullpreds, fullactuals, avg = True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
